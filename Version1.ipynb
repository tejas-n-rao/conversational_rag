{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO:\n",
    "\n",
    "# Improve model and gain insights via Evaluation metrics\n",
    "# Host on AWS\n",
    "# Improve conversational answers\n",
    "# Course codes in source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # Loading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")  # spec instance\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))  # pinecone object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing Indices on pinecone\n",
    "index_name = \"courses-ds\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# embedding model\n",
    "embedding_model = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve relevant chunks from pinecone vector database\n",
    "def retrieve_from_pinecone(\n",
    "    query, top_k=10\n",
    "):  # top_k indicates how many chunks we want to retrive from database\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    results = index.query(\n",
    "        vector=[query_embedding], top_k=top_k, include_metadata=True\n",
    "    )  # querying database\n",
    "\n",
    "    relevant_chunks = [match[\"metadata\"].get(\"text\") for match in results[\"matches\"]]\n",
    "\n",
    "    # Returning relevant chunks\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm instance\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.3,  # Experiment with different Temperatures\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable storing how many messages (10) will be remembered (Human + Bot): For conversationalness\n",
    "memory_messages = 10  # Update to change context lengths\n",
    "\n",
    "\n",
    "# Converting query to a prompt that has context from the documents\n",
    "def query_to_prompt(query, state_messages):\n",
    "    relevant_chunks = retrieve_from_pinecone(\n",
    "        query\n",
    "    )  # retrieveing relevant chunks to make context\n",
    "    context = \"\\n\\n -------------- \\n\\n\".join(chunk for chunk in relevant_chunks)\n",
    "    context = f\"\\n\\nCONTEXT: {context}\"\n",
    "\n",
    "    # Play around with this.\n",
    "    sys_message_template = (\n",
    "        \"Imagine you are a helpful assistant at Krea University who students come to clarify doubts regarding the university's Data Science curriculum. \"\n",
    "        \"Answer the query only based on the context provided. Think step by step before providing a detailed answer. \"\n",
    "        \"If you can't answer the query from the context, say that you can't. Do not hallucinate.\\n\"\n",
    "        \"The CONTEXT is as follows:\\n{}\"\n",
    "    )\n",
    "    formatted_sys_message = SystemMessage(sys_message_template.format(context))\n",
    "\n",
    "    # Remove any existing SystemMessage from state_messages\n",
    "    messages = [msg for msg in state_messages if not isinstance(msg, SystemMessage)]\n",
    "\n",
    "    # Start with the last 'memory_messages' messages from the state\n",
    "    if len(messages) > memory_messages - 1:\n",
    "        messages = messages[\n",
    "            -(memory_messages - 1) :\n",
    "        ]  # Reserve one spot for SystemMessage\n",
    "\n",
    "    # Insert the new SystemMessage at the beginning\n",
    "    messages.insert(0, formatted_sys_message)\n",
    "\n",
    "    # Add the current user query\n",
    "    messages.append(HumanMessage(content=query))\n",
    "\n",
    "    return {\"messages\": messages}  # returning the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call Rag Model\n",
    "def call_rag_model(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "\n",
    "    # Update the state with the new response\n",
    "    state[\"messages\"].append(response)\n",
    "\n",
    "    # Keep only the last few messages in the state\n",
    "    if len(state[\"messages\"]) > memory_messages:\n",
    "        state[\"messages\"] = state[\"messages\"][\n",
    "            -1 * (memory_messages) :\n",
    "        ]  # Update this for more context\n",
    "\n",
    "    # Maybe limit the context to say 1k tokens?? Maybe use an LLM or to summarise contexts?\n",
    "\n",
    "    return {\"messages\": state[\"messages\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New StageGraph from LangGraph for memory\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_rag_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uuid for thread_id for configurable\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "# print(thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting response from llm\n",
    "def get_response(query, state):\n",
    "    input_data = query_to_prompt(query, state[\"messages\"])\n",
    "    output = app.invoke(input_data, config)\n",
    "\n",
    "    # updating state history\n",
    "    state[\"messages\"] = output[\"messages\"]\n",
    "\n",
    "    llm_response = output[\"messages\"][\n",
    "        -1\n",
    "    ]  # modified very slightly in streamlit deployment to avoid excess ui-outputs\n",
    "\n",
    "    return llm_response, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_conversation(query, state):\n",
    "    llm_response, state = get_response(query, state)\n",
    "\n",
    "    print(f\"QUERY: {query}\\n\")\n",
    "    llm_response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\"messages\": []}  # Initialising Message State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: Hi, I am XYZ\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello XYZ! How can I assist you today regarding the Data Science curriculum at Krea University?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi, I am XYZ\"\n",
    "print_conversation(query, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: What is my name?\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I can't provide that information. You mentioned your name as XYZ. How can I assist you further regarding the Data Science curriculum?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"  # Checking Conversational nature\n",
    "print_conversation(query, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: Describe the Natural Language Processing course\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The Natural Language Processing (NLP) course at Krea University is a 4-credit course offered in Trimester 10. It provides a theoretical and methodological introduction to NLP, focusing on various aspects such as:\n",
      "\n",
      "1. **Basic Language Models**: Understanding foundational models used in NLP.\n",
      "2. **Part-of-Speech (POS) Tagging**: Techniques for identifying the grammatical parts of words in sentences.\n",
      "3. **Syntactic Parsing**: Analyzing the structure of sentences.\n",
      "4. **Semantic Analysis**: Understanding the meaning of words and sentences.\n",
      "\n",
      "The course covers statistical and machine learning approaches, including Hidden Markov Models and Recurrent Neural Networks, for various NLP tasks/applications such as:\n",
      "\n",
      "- Machine Translation\n",
      "- Sentiment Analysis\n",
      "- Question Answering\n",
      "- Information Extraction\n",
      "\n",
      "Additionally, the course emphasizes practical implementation and hands-on experience with algorithms using NLP toolkits, Keras, and TensorFlow libraries in Python.\n",
      "\n",
      "### Learning Outcomes:\n",
      "Upon completion of the course, students will be able to:\n",
      "- Understand how machines analyze and interpret natural language.\n",
      "- Grasp syntactic and semantic approaches in NLP.\n",
      "- Apply statistical and machine learning methods for various NLP tasks.\n",
      "- Implement NLP algorithms using relevant toolkits in Python.\n",
      "\n",
      "### Syllabus:\n",
      "The course includes the following modules:\n",
      "1. Introduction to Language Models: N-gram models and parameter estimation.\n",
      "2. Part of Speech (POS) tagging, syntactic parsing, and semantic analysis.\n",
      "3. Statistical and Machine Learning/Deep Learning approaches for NLP: Hidden Markov Models, Unsupervised Methods, and Recurrent Neural Networks.\n",
      "4. NLP Applications: Text Classification, Text Generation and Summarization, Information Extraction, Question Answering, Chatbots, Machine Translation, and Text to Speech (TTS).\n",
      "\n",
      "### Textbooks:\n",
      "Key textbooks for the course include:\n",
      "- \"Speech and Language Processing\" by Jurafsky and Martin.\n",
      "- \"Foundations of Statistical Natural Language Processing\" by Manning and Sch√ºtze.\n",
      "- \"Hands-On Natural Language Processing with Python\" by Rajesh Arumugam and Rajalingappaa Shanmugamani.\n",
      "\n",
      "This course is designed to equip students with both theoretical knowledge and practical skills in NLP, preparing them for advanced applications in the field.\n"
     ]
    }
   ],
   "source": [
    "query = \"Describe the Natural Language Processing course\"\n",
    "print_conversation(query, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Questions from Questions.txt document for evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
